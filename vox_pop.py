# %%
!pip install gensim
!pip install --upgrade pandas
!pip install --upgrade transformers accelerate sentencepiece


# %%
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
import unicodedata
import psycopg2
from sqlalchemy import create_engine, Column, Integer, String, Date, ForeignKey, text
from sqlalchemy.orm import declarative_base, sessionmaker, relationship
from sklearn.model_selection import train_test_split
from gensim.models import Word2Vec
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.cluster import KMeans
import torch
import torch.nn as nn
from collections import Counter
from sklearn.linear_model import LogisticRegression 

# %% [markdown]
# ## Task - 1

# %%
cols = ['sentiment','id','date','query_string','user','text']
df = pd.read_csv('training.1600000.processed.noemoticon.csv',
                        header=None, names=cols,encoding="latin-1")
df.info()

# %%
# Extracts the timezone part (3-4 uppercase letters) from the string
timezones = df['date'].str.extract(r'([A-Z]{3,4})')[0].unique()

print("Timezones found:", timezones)

# Counts occurrences of each timezone abbreviation
tz_counts = df['date'].str.extract(r'([A-Z]{3,4})')[0].value_counts()

print(tz_counts)
# this ensures that all the pdt is the only timezone 
# and the coutries fall under those timezones can be used to make dim_locations

# %%
df['date'] = df['date'].str.replace(r'\b[A-Z]{3}\b', '', regex=True)

df['new_date'] = pd.to_datetime(
    df['date'],
    format='%a %b %d %H:%M:%S %Y',
    errors='coerce'
)


# %%
print(df[['date', 'new_date']].head())
print(df['new_date'].isna().sum())


# %%
def clean_data(text, keep_numbers=False):
    """Clean raw text by removing noise, URLs, emails, mentions, and special characters.

    Args:
        text (str): Raw input text to clean.
        keep_numbers (bool): If True, keeps numeric characters. Default False.

    Returns:
        str: Cleaned, lowercased text string.
    """
    if not isinstance(text, str):
        return ""

    # normalize unicode
    text = unicodedata.normalize("NFKC", text)

    # remove emails
    text = re.sub(r'\S+@\S+\.\S+', ' ', text)

    # remove mentions
    text = re.sub(r'@\w+', ' ', text)

    # remove urls
    text = re.sub(r'https?://\S+|www\.\S+', ' ', text)

    # convert hashtags to just the word (keep the token)
    text = re.sub(r'#([A-Za-z0-9_]+)', r'\1', text)

    # keep alphanum and spaces (optionally keep numbers)
    if keep_numbers:
        text = re.sub(r'[^0-9A-Za-z\s]', ' ', text)
    else:
        text = re.sub(r'[^A-Za-z\s]', ' ', text)

    # collapse repeated punctuation/characters and whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # lowercase
    text = text.lower()

    return text

# %%
df['clean_text'] = df['text'].apply(clean_data)
df['target'] = df['sentiment'].apply(lambda x: 1 if x == 4 else 0)
df.head()

# %%
df.info()

# %%
print(df.duplicated().sum())
print(df.isna().sum())

# %%
# pip install psycopg2-binary

# %%
# connecting wil postgres

import os

engine = create_engine(
    os.environ.get("VOXPOP_DB_URL", "postgresql+psycopg://postgres:YOUR_PASSWORD@127.0.0.1:5432/voxpop")
)

# %%
try:
    with engine.connect() as conn:
        print("‚úÖ SQL Alchemy connected to Vox Pop")
except Exception as e:
    print("‚ùå Connection failed:", e)

# %%

# drop_tables_sql = """
# DROP TABLE IF EXISTS fact_reviews CASCADE;
# DROP TABLE IF EXISTS dim_users CASCADE;
# DROP TABLE IF EXISTS dim_sentiment CASCADE;
# """

# with engine.begin() as conn:
#     conn.execute(text(drop_tables_sql))

# print("üóëÔ∏è Existing tables deleted (if they existed)")

# %%
#  DROP TABLES IN SAFE ORDER
from sqlalchemy import text

with engine.begin() as conn:
    conn.execute(text("DROP TABLE IF EXISTS fact_reviews;"))
    conn.execute(text("DROP TABLE IF EXISTS dim_users;"))
    conn.execute(text("DROP TABLE IF EXISTS dim_sentiment;"))

print(" Old tables dropped")


#  DIM USERS
dim_users = df[['user']].drop_duplicates().reset_index(drop=True)
dim_users['user_id'] = dim_users.index + 1

dim_users.to_sql(
    'dim_users',
    con=engine,
    if_exists='replace',
    index=False
)

print(" dim_users loaded")


# 2. FACT PREP
df_merged = df.merge(dim_users, on='user', how='left')

df_merged['new_date'] = pd.to_datetime(df_merged['new_date'], errors='coerce')

fact_reviews = df_merged[['id', 'clean_text', 'sentiment', 'new_date', 'user_id']]
fact_reviews = fact_reviews.rename(columns={'id': 'review_id'})
fact_reviews = fact_reviews.drop_duplicates(subset=['review_id'])


#  FACT TABLE
fact_reviews.to_sql(
    'fact_reviews',
    con=engine,
    if_exists='replace',
    index=False,
    chunksize=5000
)

print(" fact_reviews loaded")


#  DIM SENTIMENT
sentiment_data = {
    'sentiment_id': [0, 4],
    'label_name': ['Negative', 'Positive']
}

dim_sentiment = pd.DataFrame(sentiment_data)

dim_sentiment.to_sql(
    'dim_sentiment',
    con=engine,
    if_exists='replace',
    index=False
)

print(" dim_sentiment loaded")
print(" Star Schema done")

# %%
# As we do not have have location data so I am going to skip it.(Dim_location skipped)

# %% [markdown]
# ## Task - 2

# %% [markdown]
# ##### TFIDF + Log Reg(Classical ML baseline model using Supervised learning) 

# %%
sentiment_counts = df['sentiment'].value_counts()

# %%
# spliting the data
X = df['clean_text']
y = df['target'] # target column is sentiment column

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)


# %%
from sklearn.feature_extraction.text import TfidfVectorizer

# training the model
tfidf = TfidfVectorizer(max_features=20000, stop_words='english', ngram_range=(1,2))

X_train_vect = tfidf.fit_transform(X_train)
X_test_vect = tfidf.transform(X_test)


# %%
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_vect, y_train)


# %%
# testing the model
y_pred = log_reg.predict(X_test_vect)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))


# confusion metrics
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('What the Robot Predicted')
plt.ylabel('What was Actually True')
plt.title('The Judge\'s Scorecard (Confusion Matrix)')
plt.show()

# %%
# In Task 2 I built a classical machine learning baseline.
# First I converted text into numeric features using TF-IDF.
# Then I trained a Logistic Regression classifier to predict sentiment.
# I split the dataset into train and test to avoid data leakage.
# After training, I evaluated the model using accuracy, precision, recall, F1 score and a confusion matrix.


# %% [markdown]
# Un-Supervised Learning

# %%
sample_df = df.sample(50000, random_state=42)

# %%
tfidf_cluster = TfidfVectorizer(
    max_features=20000,
    stop_words='english',
    ngram_range=(1,2)
)

X_cluster = tfidf_cluster.fit_transform(sample_df['clean_text'])


# %%
inertia = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_cluster)
    inertia.append(kmeans.inertia_)

plt.plot(K_range, inertia, 'bx-')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

# %%
# The curve clearly bends at 6 after it improvements are minor.

kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)
kmeans.fit(X_cluster)

# %%
terms = tfidf_cluster.get_feature_names_out()
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]

for i in range(6):
    print(f"\nCluster {i}:")
    for ind in order_centroids[i, :8]:
        print(terms[ind])


# %%
X_all_cluster = tfidf_cluster.transform(df['clean_text'])
df['cluster'] = kmeans.predict(X_all_cluster)


# %%
pd.crosstab(df['cluster'], df['sentiment'])


# %%
sns.countplot(x='cluster', data=df)
plt.title('Cluster distribution')
plt.show()


# %%
cluster_sent = pd.crosstab(df['cluster'], df['sentiment'])

sns.heatmap(cluster_sent, annot=True, fmt='d')
plt.title('Sentiment by Cluster')
plt.show()


# %%
# In Task 2 I built a supervised baseline using TF-IDF and Logistic Regression to predict sentiment.
#  Then I used K-Means clustering to discover hidden topics.
#  By comparing clusters with sentiment, I identified which topics generate complaints versus positive feedback.

# %%
# The dataset did not contain geographical information, so regional EDA could not be performed. 
# If location data becomes available, the same pivot-table analysis can be applied.

# %% [markdown]
# ## Task - 3

# %%
# Text ‚Üí sequence ‚Üí LSTM ‚Üí emotion score
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# %%
from collections import Counter

words = " ".join(df['clean_text']).split()
vocab = Counter(words)

vocab_to_int = {word:i+1 for i,(word,_) in enumerate(vocab.most_common(20000))}


# %%
def encode(text):
    """Encode text into integer sequence using vocabulary mapping.

    Args:
        text (str): Cleaned text to encode.

    Returns:
        list[int]: List of integer-encoded word indices.
    """
    return [vocab_to_int[word] for word in text.split() if word in vocab_to_int]

df['encoded'] = df['clean_text'].apply(encode)


# %%
max_len = 50
def pad(seq, max_len):
    """Pad or truncate a sequence to a fixed length.

    Args:
        seq (list[int]): Input integer sequence.
        max_len (int): Target length for padding/truncating.

    Returns:
        list[int]: Sequence of exactly max_len length.
    """
    if len(seq) >= max_len:
        return seq[:max_len]
    return seq + [0] * (max_len - len(seq))

X_seq = np.array([pad(s, max_len) for s in df['encoded']])


# %%


# %%
X_train, X_test, y_train, y_test = train_test_split(
    X_seq, df['target'],
    test_size=0.2,
    random_state=42
)


# %%
import torch

X_train = torch.tensor(X_train, dtype=torch.long).to(device)
y_train = torch.tensor(y_train.values, dtype=torch.float32).to(device)

X_test = torch.tensor(X_test, dtype=torch.long).to(device)
y_test = torch.tensor(y_test.values, dtype=torch.float32).to(device)


# %%
from torch.utils.data import TensorDataset, DataLoader

train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size=256, shuffle=True)

# %%
import torch.nn as nn

class LSTMModel(nn.Module):
    """LSTM-based neural network for binary sentiment classification.

    Architecture: Embedding ‚Üí LSTM ‚Üí Fully Connected ‚Üí Sigmoid.
    """
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64):
        """Initialize the LSTM model.

        Args:
            vocab_size (int): Size of the vocabulary.
            embed_dim (int): Dimension of word embeddings. Default 128.
            hidden_dim (int): Number of LSTM hidden units. Default 64.
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """Forward pass through the LSTM network.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, seq_len).

        Returns:
            torch.Tensor: Sigmoid probability output of shape (batch_size, 1).
        """
        x = self.embedding(x)
        _, (hidden, _) = self.lstm(x)
        out = self.fc(hidden[-1])
        return self.sigmoid(out)


# %%


# %%
model = LSTMModel(len(vocab_to_int)+1).to(device)
loss_fn = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 5

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for xb, yb in train_loader:
        xb = xb.to(device)
        yb = yb.to(device)

        preds = model(xb)
        loss = loss_fn(preds.squeeze(), yb)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} | Avg Loss: {total_loss / len(train_loader):.4f}")



# %%
test_data = TensorDataset(X_test, y_test)
test_loader = DataLoader(test_data, batch_size=256, shuffle=False)


# %%
model.eval()

correct = 0
total = 0

with torch.no_grad():
    for xb, yb in test_loader:
        xb, yb = xb.to(device), yb.to(device)

        preds = model(xb)
        preds = (preds.squeeze() > 0.5).float()

        correct += (preds == yb).sum().item()
        total += yb.size(0)

accuracy = correct / total
print("Test Accuracy:", accuracy)


# %%
from sklearn.metrics import classification_report

all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device)
        preds = model(xb)
        preds = (preds.squeeze() > 0.5).cpu().numpy()

        all_preds.extend(preds)
        all_labels.extend(yb.cpu().numpy())

print(classification_report(all_labels, all_preds))


# %%
torch.cuda.empty_cache()


# %%
# Deep Learning Emotional Modeling

# A PyTorch LSTM neural network was implemented to capture emotional patterns in sequential text. 
# Reviews were encoded into padded numeric sequences and trained using GPU-accelerated mini-batch gradient descent.

# Training loss consistently decreased, demonstrating stable learning.
#  The final model achieved approximately 83% accuracy and F1-score on unseen test data, with balanced precision and recall across both sentiment classes.

# Batched inference was used during evaluation to prevent GPU memory overflow and ensure scalable deployment. 
# The results confirm that sequence-based deep learning models outperform classical bag-of-words baselines by capturing contextual emotional signals.

# %% [markdown]
# ## Task - 4

# %%

# %%
import transformers
print(transformers.__version__)


# %%
import sys
print(sys.executable)


# %%
from transformers import BartTokenizer, BartForConditionalGeneration, AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

model_name = "facebook/bart-large-cnn"

tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

clf_name = "cardiffnlp/twitter-roberta-base-sentiment"

clf_tokenizer = AutoTokenizer.from_pretrained(clf_name)
clf_model = AutoModelForSequenceClassification.from_pretrained(clf_name)
clf_model.to(device)

# %%
# from sqlalchemy import create_engine, text

# engine = create_engine(
#     os.environ.get("VOXPOP_DB_URL", "postgresql+psycopg://postgres:YOUR_PASSWORD@127.0.0.1:5432/voxpop")
# )


# query = text("""
# SELECT table_name
# FROM information_schema.tables
# WHERE table_schema = 'public';
# """)

# with engine.connect() as conn:
#     tables = conn.execute(query).fetchall()

# print(tables)



# %%
from sqlalchemy import create_engine, text
from transformers import (
    BartTokenizer,
    BartForConditionalGeneration,
    AutoTokenizer,
    AutoModelForSequenceClassification
)
import torch
import torch.nn.functional as F
from datetime import datetime

from transformers import AutoModelForTokenClassification
from transformers import pipeline as hf_pipeline

#  Load NER model
ner_name = "dslim/bert-base-NER"

ner_tokenizer = AutoTokenizer.from_pretrained(ner_name)
ner_model = AutoModelForTokenClassification.from_pretrained(ner_name)

ner = hf_pipeline(
    "ner",
    model=ner_model,
    tokenizer=ner_tokenizer,
    aggregation_strategy="simple",
    device=0 if torch.cuda.is_available() else -1

)



# CONFIG 
MAX_CHUNK_CHARS = 2500

engine = create_engine(
    os.environ.get("VOXPOP_DB_URL", "postgresql+psycopg://postgres:YOUR_PASSWORD@127.0.0.1:5432/voxpop")
)

#  Load BART
model_name = "facebook/bart-large-cnn"

tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#  Load complaint classifier 
clf_name = "cardiffnlp/twitter-roberta-base-sentiment"

clf_tokenizer = AutoTokenizer.from_pretrained(clf_name)
clf_model = AutoModelForSequenceClassification.from_pretrained(clf_name)
clf_model.to(device)


def is_complaint(text_):
    """Classify whether text is a complaint using a pre-trained sentiment classifier.

    Args:
        text_ (str): Review text to classify.

    Returns:
        bool: True if classified as complaint (>70% confidence).
    """
    inputs = clf_tokenizer(
        text_,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)

    outputs = clf_model(**inputs)
    probs = F.softmax(outputs.logits, dim=1)

    return probs[0][0] > 0.7


# Topic grouping
def group_by_topic(reviews):
    """Group reviews into topic categories using keyword matching.

    Args:
        reviews (list[str]): List of review texts.

    Returns:
        dict[str, list[str]]: Reviews grouped by topic.
    """
    topics = {
        "browser": [],
        "mobile": [],
        "website": [],
        "system": [],
        "other": []
    }

    for r in reviews:
        t = r.lower()

        if "firefox" in t or "browser" in t:
            topics["browser"].append(r)

        elif "phone" in t or "mobile" in t or "app" in t:
            topics["mobile"].append(r)

        elif "website" in t or "server" in t:
            topics["website"].append(r)

        elif "windows" in t or "driver" in t or "computer" in t:
            topics["system"].append(r)

        else:
            topics["other"].append(r)

    return topics


def is_business_issue(text_):
    """Check if text contains business-related keywords.

    Args:
        text_ (str): Text to check.

    Returns:
        bool: True if business keywords found.
    """
    keywords = [
        "app", "website", "server", "login", "payment",
        "error", "bug", "crash", "down", "support",
        "account", "update", "device", "service"
    ]

    t = text_.lower()

    return any(k in t for k in keywords)


#  Fetch negative reviews
def fetch_negative_reviews(limit=1000):
    """Fetch and filter negative reviews from the database.

    Args:
        limit (int): Max reviews to fetch. Default 1000.

    Returns:
        list[str]: Filtered complaint texts.
    """
    query = text("""
        SELECT clean_text
        FROM fact_reviews
        WHERE sentiment = 0
        AND LENGTH(clean_text) > 40
        LIMIT :limit
    """)

    with engine.connect() as conn:
        rows = conn.execute(query, {"limit": limit}).fetchall()

    reviews = list(set([r[0] for r in rows if r[0]]))

    filtered = []

    print("Running complaint classifier...")

    for text_ in reviews:
        if is_complaint(text_) and is_business_issue(text_):
            filtered.append(text_)


    return filtered


#  Chunk long text
def chunk_text(text, chunk_size=MAX_CHUNK_CHARS):
    """Split text into chunks for summarization.

    Args:
        text (str): Input text.
        chunk_size (int): Max characters per chunk.

    Returns:
        list[str]: Text chunks.
    """
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]


#  Summarize single chunk
def summarize_text(text):
    """Summarize a single text chunk using BART.

    Args:
        text (str): Text to summarize.

    Returns:
        str: Generated summary.
    """
    inputs = tokenizer(
        text,
        max_length=1024,
        return_tensors="pt",
        truncation=True
    ).to(device)

    summary_ids = model.generate(
        inputs["input_ids"],
        num_beams=4,
        max_length=120,
        early_stopping=True
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)


# Summarize all chunks
def summarize_chunks(chunks):
    """Summarize multiple chunks and combine results.

    Args:
        chunks (list[str]): Text chunks.

    Returns:
        str: Combined summary.
    """
    summaries = []

    for i, chunk in enumerate(chunks, 1):
        print(f"Summarizing chunk {i}/{len(chunks)}...")
        summaries.append(summarize_text(chunk))

    return " ".join(summaries)


# Executive compression
def compress_summary(text):
    """Compress summary into a concise executive brief using BART.

    Args:
        text (str): Summary to compress.

    Returns:
        str: Compressed brief (max 80 tokens).
    """
    print("Running executive compression...")

    inputs = tokenizer(
        text,
        max_length=1024,
        return_tensors="pt",
        truncation=True
    ).to(device)

    summary_ids = model.generate(
        inputs["input_ids"],
        num_beams=6,
        max_length=80,
        early_stopping=True
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)


# KPI: Mean Time to Detect (MTTD)
# Simulates a crisis detection scenario and compares AI vs Human speed
def calculate_mttd():
    """Calculate Mean Time to Detect for crisis events.

    Measures how fast the AI pipeline detects crises compared
    to an estimated 2-hour human baseline. Goal: 10x faster.
    """
    import time
    print("üö® Simulating Crisis Detection...\n")
    start = time.time()
    # --- AI Crisis Detection Pipeline ---
    reviews = fetch_negative_reviews()
    topics = group_by_topic(reviews)
    for name, texts in topics.items():
        if not texts:
            continue
        combined = " ".join(texts)
        chunks = chunk_text(combined)
        summary = summarize_chunks(chunks)
        summary = compress_summary(summary)
    ai_time = time.time() - start
    # --- Human Estimate ---
    # A human moderator manually reading 1000+ reviews, grouping,
    # and writing a summary would take ~2 hours conservatively
    human_estimate_minutes = 120
    speedup = (human_estimate_minutes * 60) / ai_time
    print(f"  AI Detection Time:    {ai_time:.1f} seconds")
    print(f" Human Estimate:       {human_estimate_minutes} minutes ({human_estimate_minutes * 60}s)")
    print(f" Speedup:              {speedup:.0f}√ó faster")
    print(f"\n Goal (10√ó faster):    {'ACHIEVED ' if speedup >= 10 else 'NOT MET '}")


# KPI: Summarization Compression Ratio
# Measures how effectively BART condenses feedback while preserving entities
def calculate_compression_ratio():
    """Measure the summarization compression ratio per topic.

    Calculates how effectively BART condenses feedback while
    preserving entity mentions. Reports input/output word counts.
    """
    reviews = fetch_negative_reviews()
    topics = group_by_topic(reviews)
    total_input_words = 0
    total_output_words = 0
    print("üìê Compression Ratio per Topic:\n")
    print(f"{'Topic':<20} {'Input Words':>12} {'Output Words':>13} {'Ratio':>8}")
    print("‚îÄ" * 58)
    for name, texts in topics.items():
        if not texts:
            continue
        combined = " ".join(texts)
        input_words = len(combined.split())
        chunks = chunk_text(combined)
        summary = summarize_chunks(chunks)
        summary = compress_summary(summary)
        output_words = len(summary.split())
        ratio = input_words / output_words if output_words > 0 else 0
        total_input_words += input_words
        total_output_words += output_words
        print(f"{name:<20} {input_words:>12,} {output_words:>13,} {ratio:>7.0f}:1")
    # Overall
    overall_ratio = total_input_words / total_output_words if total_output_words > 0 else 0
    print("‚îÄ" * 58)
    print(f"{'OVERALL':<20} {total_input_words:>12,} {total_output_words:>13,} {overall_ratio:>7.0f}:1")
    print(f"\n BART compressed {total_input_words:,} words ‚Üí {total_output_words:,} words ({overall_ratio:.0f}:1 ratio)")




# Store crisis report
def store_summary(summary_text):
    """Store a crisis summary in the database.

    Args:
        summary_text (str): Generated crisis summary.

    Returns:
        int: Database ID of the stored crisis report.
    """
    create_table = text("""
        CREATE TABLE IF NOT EXISTS crisis_reports (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP,
            summary TEXT
        )
    """)

    insert_query = text("""
        INSERT INTO crisis_reports (timestamp, summary)
        VALUES (:timestamp, :summary)
        RETURNING id
    """)

    with engine.begin() as conn:
        conn.execute(create_table)
        result = conn.execute(insert_query, {
            "timestamp": datetime.now(),
            "summary": summary_text
        })

        crisis_id = result.scalar()

    return crisis_id


def store_entities(summary_text, crisis_id):
    """Extract and store named entities from a crisis summary.

    Args:
        summary_text (str): Crisis summary to extract entities from.
        crisis_id (int): Associated crisis report ID.
    """
    create_table = text("""
        CREATE TABLE IF NOT EXISTS entity_store (
            id SERIAL PRIMARY KEY,
            crisis_id INTEGER,
            entity TEXT,
            label TEXT
        )
    """)

    entities = ner(summary_text)

    with engine.begin() as conn:
        conn.execute(create_table)

        for ent in entities:
            conn.execute(text("""
                INSERT INTO entity_store (crisis_id, entity, label)
                VALUES (:cid, :ent, :lab)
            """), {
                "cid": crisis_id,
                "ent": ent["word"],
                "lab": ent["entity_group"]
            })


# Main pipeline
def run_crisis_summary():
    """Execute the full crisis detection and summarization pipeline.

    Fetches negative reviews, groups by topic, summarizes each group,
    stores the report, and extracts entities.
    """
    print("Fetching negative reviews...")
    reviews = fetch_negative_reviews()

    if not reviews:
        print("No negative reviews found.")
        return

    topics = group_by_topic(reviews)

    final_reports = []

    for name, texts in topics.items():
        if not texts:
            continue

        combined = " ".join(texts)
        chunks = chunk_text(combined)

        print(f"\nTopic: {name.upper()} | chunks: {len(chunks)}")

        summary = summarize_chunks(chunks)
        summary = compress_summary(summary)

        final_reports.append(f"[{name.upper()}] {summary}")

    final_summary = "\n".join(final_reports)

    
    crisis_id = store_summary(final_summary)
    store_entities(final_summary, crisis_id)

    print("\nCrisis Summary Generated:\n")
    print(final_summary)


#  Execute 
if __name__ == "__main__":
    run_crisis_summary()


# %%
from sqlalchemy import create_engine, text
from transformers import ( BartTokenizer, BartForConditionalGeneration, AutoTokenizer, 
                          AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline as hf_pipeline)
import torch
import torch.nn.functional as F
from datetime import datetime

# CONFIG 
MAX_CHUNK_CHARS = 2500

engine = create_engine(
    os.environ.get("VOXPOP_DB_URL", "postgresql+psycopg://postgres:YOUR_PASSWORD@127.0.0.1:5432/voxpop")
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load BART summarizer 
model_name = "facebook/bart-large-cnn"

tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)
model.to(device)

# Load complaint classifier
clf_name = "cardiffnlp/twitter-roberta-base-sentiment"

clf_tokenizer = AutoTokenizer.from_pretrained(clf_name)
clf_model = AutoModelForSequenceClassification.from_pretrained(clf_name)
clf_model.to(device)

#  Load NER model
ner_name = "dslim/bert-base-NER"

ner_tokenizer = AutoTokenizer.from_pretrained(ner_name)
ner_model = AutoModelForTokenClassification.from_pretrained(ner_name)

ner = hf_pipeline(
    "ner",
    model=ner_model,
    tokenizer=ner_tokenizer,
    aggregation_strategy="simple",
    device=0 if torch.cuda.is_available() else -1
)


# %%
def is_complaint(text_):
    inputs = clf_tokenizer(
        text_,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)

    outputs = clf_model(**inputs)
    probs = F.softmax(outputs.logits, dim=1)

    return probs[0][0] > 0.7


def group_by_topic(reviews):
    topics = {
        "browser": [],
        "mobile": [],
        "website": [],
        "system": [],
        "other": []
    }

    for r in reviews:
        t = r.lower()

        if "firefox" in t or "browser" in t:
            topics["browser"].append(r)

        elif "phone" in t or "mobile" in t or "app" in t:
            topics["mobile"].append(r)

        elif "website" in t or "server" in t:
            topics["website"].append(r)

        elif "windows" in t or "driver" in t or "computer" in t:
            topics["system"].append(r)

        else:
            topics["other"].append(r)

    return topics


def is_business_issue(text_):
    keywords = [
        "app", "website", "server", "login", "payment",
        "error", "bug", "crash", "down", "support",
        "account", "update", "device", "service"
    ]

    t = text_.lower()
    return any(k in t for k in keywords)


def fetch_negative_reviews(limit=1000):
    query = text("""
        SELECT clean_text
        FROM fact_reviews
        WHERE sentiment = 0
        AND LENGTH(clean_text) > 40
        LIMIT :limit
    """)

    with engine.connect() as conn:
        rows = conn.execute(query, {"limit": limit}).fetchall()

    reviews = list(set([r[0] for r in rows if r[0]]))

    filtered = []

    print("Running complaint classifier...")

    for text_ in reviews:
        if is_complaint(text_) and is_business_issue(text_):
            filtered.append(text_)

    return filtered


def chunk_text(text, chunk_size=MAX_CHUNK_CHARS):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]


def summarize_text(text):
    inputs = tokenizer(
        text,
        max_length=1024,
        return_tensors="pt",
        truncation=True
    ).to(device)

    summary_ids = model.generate(
        inputs["input_ids"],
        num_beams=4,
        max_length=120,
        early_stopping=True
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)


def summarize_chunks(chunks):
    summaries = []

    for i, chunk in enumerate(chunks, 1):
        print(f"Summarizing chunk {i}/{len(chunks)}...")
        summaries.append(summarize_text(chunk))

    return " ".join(summaries)


def compress_summary(text):
    print("Running executive compression...")

    inputs = tokenizer(
        text,
        max_length=1024,
        return_tensors="pt",
        truncation=True
    ).to(device)

    summary_ids = model.generate(
        inputs["input_ids"],
        num_beams=6,
        max_length=80,
        early_stopping=True
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)


def store_summary(summary_text):
    create_table = text("""
        CREATE TABLE IF NOT EXISTS crisis_reports (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP,
            summary TEXT
        )
    """)

    insert_query = text("""
        INSERT INTO crisis_reports (timestamp, summary)
        VALUES (:timestamp, :summary)
        RETURNING id
    """)

    with engine.begin() as conn:
        conn.execute(create_table)
        result = conn.execute(insert_query, {
            "timestamp": datetime.now(),
            "summary": summary_text
        })

        crisis_id = result.scalar()

    return crisis_id


def store_entities(summary_text, crisis_id):
    create_table = text("""
        CREATE TABLE IF NOT EXISTS entity_store (
            id SERIAL PRIMARY KEY,
            crisis_id INTEGER,
            entity TEXT,
            label TEXT
        )
    """)

    entities = ner(summary_text)

    with engine.begin() as conn:
        conn.execute(create_table)

        for ent in entities:
            conn.execute(text("""
                INSERT INTO entity_store (crisis_id, entity, label)
                VALUES (:cid, :ent, :lab)
            """), {
                "cid": crisis_id,
                "ent": ent["word"],
                "lab": ent["entity_group"]
            })


# %%
def run_crisis_summary():
    print("Fetching negative reviews...")
    reviews = fetch_negative_reviews()

    if not reviews:
        print("No negative reviews found.")
        return

    topics = group_by_topic(reviews)
    final_reports = []

    for name, texts in topics.items():
        if not texts:
            continue

        combined = " ".join(texts)
        chunks = chunk_text(combined)

        print(f"\nTopic: {name.upper()} | chunks: {len(chunks)}")

        summary = summarize_chunks(chunks)
        summary = compress_summary(summary)

        final_reports.append(f"[{name.upper()}] {summary}")

    final_summary = "\n".join(final_reports)

    crisis_id = store_summary(final_summary)
    store_entities(final_summary, crisis_id)

    print("\nCrisis Summary Generated:\n")
    print(final_summary)


run_crisis_summary()


# %%
# The crisis summary reflects the dominant negative signals present in the dataset. 
# Since the dataset contains general emotional distress and technical frustrations rather 
# than brand-specific complaints, the generated report captures personal and technical crisis patterns 
# instead of product failures.


# %% [markdown]
# NER code

# %%
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline as ner_pipeline


# %%
ner_name = "dslim/bert-base-NER"
ner_tokenizer = AutoTokenizer.from_pretrained(ner_name)
ner_model = AutoModelForTokenClassification.from_pretrained(ner_name)
ner = ner_pipeline(
    "ner",
    model=ner_model,
    tokenizer=ner_tokenizer,
    aggregation_strategy="simple"
)

# %%
# for checking purpose whether NER is working or not?
summary_text = """
Users report that the Apple iPhone app crashes in New York.
Google support has not responded since Monday.
CEO Tim Cook was mentioned in complaints.
"""


entities = ner(summary_text)

for e in entities:
    print(e)

# %% [markdown]
# SQL Part
# 

# %%
# Top mentioned entities

query = text("""
SELECT entity, label, COUNT(*) AS mentions
FROM entity_store
GROUP BY entity, label
ORDER BY mentions DESC
LIMIT 10;
""")

df = pd.read_sql(query, engine)
df


# %%
# Crisis history timeline

query = text("""
SELECT DATE(timestamp) AS day, COUNT(*) AS reports
FROM crisis_reports
GROUP BY day
ORDER BY day;
""")

pd.read_sql(query, engine)



# %%
# Entities per crisis
query = text("""
SELECT crisis_id, COUNT(*) AS entity_count
FROM entity_store
GROUP BY crisis_id
ORDER BY entity_count DESC;
""")

pd.read_sql(query, engine)


# %%
# Evaluation Metrics ‚Äî Pillar 1: Technical AI Performance

def calculate_perplexity():
    """Calculate perplexity of the BART summarizer on sample text.

    Perplexity measures how 'surprised' the model is by new data.
    Lower scores indicate more fluent, human-like summaries.
    """
    import torch
    import math

    sample_texts = fetch_negative_reviews(limit=50)
    if not sample_texts:
        print("No reviews to evaluate perplexity.")
        return

    combined = " ".join(sample_texts[:10])
    inputs = tokenizer(combined, return_tensors="pt", truncation=True, max_length=1024).to(device)

    with torch.no_grad():
        outputs = model(input_ids=inputs["input_ids"], labels=inputs["input_ids"])
        loss = outputs.loss
        perplexity = math.exp(loss.item())

    print(f"üìä BART Summarizer Perplexity: {perplexity:.2f}")
    print(f"   (Lower is better ‚Äî indicates more fluent summaries)")
    return perplexity


# Evaluation Metrics ‚Äî Pillar 2: Data Engineering Efficiency

def measure_inference_latency():
    """Measure inference latency: time for raw review ‚Üí model ‚Üí result.

    Goal: < 500ms per record.
    """
    import time

    sample = fetch_negative_reviews(limit=10)
    if not sample:
        print("No reviews available for latency test.")
        return

    times = []
    for text_ in sample:
        start = time.time()
        # Simulate full pipeline: clean ‚Üí classify ‚Üí summarize
        is_complaint(text_)
        elapsed = time.time() - start
        times.append(elapsed)

    avg_ms = (sum(times) / len(times)) * 1000
    print(f"‚ö° Inference Latency (avg per record): {avg_ms:.1f}ms")
    print(f"   Goal: < 500ms ‚Äî {'ACHIEVED ‚úÖ' if avg_ms < 500 else 'NOT MET ‚ùå'}")
    return avg_ms


def check_data_integrity():
    """Check data integrity: % of records passing SQL constraints.

    Validates no null sentiments, no empty clean_text, and valid dates.
    Goal: 100% integrity.
    """
    integrity_query = text("""
        SELECT
            COUNT(*) AS total,
            SUM(CASE WHEN sentiment IS NULL THEN 1 ELSE 0 END) AS null_sentiment,
            SUM(CASE WHEN clean_text IS NULL OR clean_text = '' THEN 1 ELSE 0 END) AS empty_text,
            SUM(CASE WHEN new_date IS NULL THEN 1 ELSE 0 END) AS null_date
        FROM fact_reviews
    """)

    with engine.connect() as conn:
        row = conn.execute(integrity_query).fetchone()

    total = row[0]
    null_sentiment = row[1]
    empty_text = row[2]
    null_date = row[3]
    issues = null_sentiment + empty_text + null_date
    integrity_pct = ((total * 3 - issues) / (total * 3)) * 100

    print(f"üîí Data Integrity Report:")
    print(f"   Total Records:      {total:,}")
    print(f"   Null Sentiments:    {null_sentiment}")
    print(f"   Empty clean_text:   {empty_text}")
    print(f"   Null Dates:         {null_date}")
    print(f"   Integrity Score:    {integrity_pct:.2f}%")
    print(f"   Goal: 100% ‚Äî {'ACHIEVED ‚úÖ' if integrity_pct == 100 else 'NOT MET ‚ùå'}")
    return integrity_pct


def measure_sql_query_time():
    """Measure execution time of complex analytical SQL queries.

    Goal: Complex 'Sentiment Over Time' queries should return in < 1s.
    """
    import time

    queries = {
        "Sentiment Over Time": text("""
            SELECT DATE_TRUNC('week', new_date) AS week,
                   SUM(CASE WHEN sentiment = 4 THEN 1 ELSE 0 END) AS positive,
                   SUM(CASE WHEN sentiment = 0 THEN 1 ELSE 0 END) AS negative
            FROM fact_reviews
            WHERE new_date IS NOT NULL
            GROUP BY week
            ORDER BY week
        """),
        "Top Entities": text("""
            SELECT entity, label, COUNT(*) AS mentions
            FROM entity_store
            GROUP BY entity, label
            ORDER BY mentions DESC
            LIMIT 10
        """),
        "Net Sentiment Score": text("""
            SELECT
                ROUND(
                    (SUM(CASE WHEN sentiment = 4 THEN 1 ELSE 0 END)::numeric / COUNT(*) * 100) -
                    (SUM(CASE WHEN sentiment = 0 THEN 1 ELSE 0 END)::numeric / COUNT(*) * 100), 2
                ) AS nss
            FROM fact_reviews
        """),
    }

    print(f"‚è±Ô∏è  SQL Query Execution Times:\n")
    print(f"{'Query':<25} {'Time (ms)':>10} {'Status':>10}")
    print("‚îÄ" * 48)

    all_pass = True
    for name, q in queries.items():
        start = time.time()
        with engine.connect() as conn:
            conn.execute(q).fetchall()
        elapsed_ms = (time.time() - start) * 1000
        status = "‚úÖ" if elapsed_ms < 1000 else "‚ùå"
        if elapsed_ms >= 1000:
            all_pass = False
        print(f"{name:<25} {elapsed_ms:>8.1f}ms {status:>10}")

    print(f"\n   Goal: < 1s per query ‚Äî {'ALL ACHIEVED ‚úÖ' if all_pass else 'SOME NOT MET ‚ùå'}")


# %%
def main():
    """Main entry point ‚Äî runs the full VoxPop pipeline."""
    run_crisis_summary()

if __name__ == "__main__":
    main()





